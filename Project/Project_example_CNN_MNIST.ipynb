{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_example_MNIST.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZumuKg42lPIx",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QerY86J3l8aT",
        "colab_type": "text"
      },
      "source": [
        "## MISNT Data Classifier with Pytorch\n",
        "Example of CNN project based class task\n",
        "\n",
        "#### Content  \n",
        "Code up MNIST Deep Neural Network Classifier from scratch with various choice of \"Hyper parameters\"\n",
        "\n",
        "#### Work environment\n",
        "Google Colaboratory  \n",
        "How to use Colaboratory [Geeksforgeeks: How to use google colab](https://www.geeksforgeeks.org/how-to-use-google-colab/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sigPUomzowHw",
        "colab_type": "text"
      },
      "source": [
        "-------------\n",
        "### Import google.colab module to mount your gdrive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHOcYGq8olmA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "143f85a5-5b08-47f7-8791-41545563de7a"
      },
      "source": [
        "# Amount your drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lMd7VHbop8H",
        "colab_type": "text"
      },
      "source": [
        "-----------------\n",
        "### Import necessary modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soYseDAGmjD5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import argparse\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVIUeICDrLPl",
        "colab_type": "text"
      },
      "source": [
        "### Set parameters as argument parser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwsU1EO8q8QY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_FLAGS(verbose=True):\n",
        "  p = argparse.ArgumentParser()\n",
        "  p.add_argument(\n",
        "      '--batch_size',\n",
        "      type=int,\n",
        "      default=32          \n",
        "  )\n",
        "  p.add_argument(\n",
        "      '--lr',\n",
        "      type=float,\n",
        "      default=0.001, \n",
        "      help='learning rate'\n",
        "  )\n",
        "  p.add_argument(\n",
        "      '--num_epoch',\n",
        "      type=int,\n",
        "      default=5        \n",
        "  )\n",
        "  args = p.parse_args(args=[])\n",
        "  # + Memo\n",
        "  # in script type coding line, you should specify the above line as \n",
        "  # args = parser.parse_args()\n",
        "  if verbose:\n",
        "    for f in args.__dict__:\n",
        "      print(\"\\t\", f, (25 - len(f)) * \" \", args.__dict__[f])\n",
        "  return args"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SM3uxjusq8ht",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "25c519da-e7ac-4874-9647-c63d59ad9554"
      },
      "source": [
        "FLAGS = get_FLAGS()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t batch_size                 32\n",
            "\t lr                         0.001\n",
            "\t num_epoch                  5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfadkHr8pwQS",
        "colab_type": "text"
      },
      "source": [
        "### Load MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53jfNKjqmEp_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train dataloader\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST(\n",
        "        root='./mnist_data',\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transforms.Compose([\n",
        "            transforms.Resize((28,28)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,), (0.5,))\n",
        "        ])\n",
        "    ),\n",
        "    batch_size=FLAGS.batch_size,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "# test dataloader\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST(\n",
        "        root='./mnist_data',\n",
        "        train=False,\n",
        "        download=False,\n",
        "        transform=transforms.Compose([\n",
        "            transforms.Resize((28,28)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,), (0.5,))\n",
        "        ])\n",
        "    ),\n",
        "    batch_size=FLAGS.batch_size\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wc3OKcalPRkJ",
        "colab_type": "text"
      },
      "source": [
        "### Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68Ht7UxeNZz1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Flatten(nn.Module):\n",
        "  def forward(self, x):\n",
        "    return x.view(x.size()[0], -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wKYZ-uVqq2a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class model_fn(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(model_fn, self).__init__()\n",
        "        \n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(1,16,kernel_size=2, padding=1, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Dropout(p=0.3),\n",
        "            nn.Conv2d(16, 32, kernel_size=2, padding=1, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Dropout(p=0.3),\n",
        "            nn.Conv2d(32, 32, kernel_size=2, padding=1, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Dropout(p=0.3),\n",
        "            Flatten(),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(512, 10),\n",
        "            nn.Softmax()\n",
        "        )\n",
        "    #\n",
        "    \n",
        "    def forward(self, x):        \n",
        "        x = self.cnn(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA6iSgBx95Se",
        "colab_type": "text"
      },
      "source": [
        "### Training Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9Nyj3DJ_0i-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "862bad24-d064-4a13-ea25-c9fa89e6247f"
      },
      "source": [
        "# Show Model Architecture\n",
        "print(model_fn())"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model_fn(\n",
            "  (cnn): Sequential(\n",
            "    (0): Conv2d(1, 16, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Dropout(p=0.3)\n",
            "    (4): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
            "    (5): ReLU()\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Dropout(p=0.3)\n",
            "    (8): Conv2d(32, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
            "    (9): ReLU()\n",
            "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (11): Dropout(p=0.3)\n",
            "    (12): Flatten()\n",
            "    (13): ReLU()\n",
            "    (14): Dropout(p=0.5)\n",
            "    (15): Linear(in_features=512, out_features=10, bias=True)\n",
            "    (16): Softmax()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyvNGsSp94KQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "3bd37141-cb36-4e4f-aab7-387e515c108b"
      },
      "source": [
        "# get model\n",
        "model = model_fn()\n",
        "model = model.to('cuda')\n",
        "\n",
        "# optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=FLAGS.lr)\n",
        "criterion = nn.BCELoss().cuda()\n",
        "\n",
        "# train start\n",
        "model.train()\n",
        "for epoch in range(FLAGS.num_epoch):\n",
        "  running_loss = 0.000\n",
        "  for i, (inputs, labels) in enumerate(train_loader):\n",
        "    model.zero_grad()\n",
        "    outputs = model.cnn(inputs.cuda())\n",
        "    \n",
        "    labels_one_hot =  torch.eye(10)[labels]\n",
        "    loss = criterion(outputs.cuda(), labels_one_hot.cuda())\n",
        "    running_loss += loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "  print( \" ### Num. of Epochs {}  | Loss {} ### \".format(\n",
        "        epoch,running_loss/float(len(train_loader.dataset)//FLAGS.batch_size) ), flush=True\n",
        "  )\n",
        "    "
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " ### Num. of Epochs 0  | Loss 0.10621947050094604 ### \n",
            " ### Num. of Epochs 1  | Loss 0.05522262677550316 ### \n",
            " ### Num. of Epochs 2  | Loss 0.04756315052509308 ### \n",
            " ### Num. of Epochs 3  | Loss 0.043285928666591644 ### \n",
            " ### Num. of Epochs 4  | Loss 0.04122423008084297 ### \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZEh6R_WRmIT",
        "colab_type": "text"
      },
      "source": [
        "#### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSuoJrWuRTNi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model, test_loader):\n",
        "  model.eval()\n",
        "  correct = 0.000\n",
        "  \n",
        "  for data, target in test_loader:\n",
        "    data, target = torch.autograd.Variable(data), torch.autograd.Variable(target)\n",
        "    output = model(data.cuda())\n",
        "    \n",
        "    pred = output.data.max(1, keepdim=True)[1].cpu() # output label\n",
        "    correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "    \n",
        "  print(\"Num. of Right Answers {}/{} | Accuracy in Test data {} %\".format( correct,len(test_loader.dataset), 100. * float(correct) / len(test_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Bq7CWhXUoj-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "e776cc1f-9475-42f3-bcdb-c7b067b321e5"
      },
      "source": [
        "test(model, test_loader)"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Num. of Right Answers 9786/10000 | Accuracy in Test data 97.86 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}